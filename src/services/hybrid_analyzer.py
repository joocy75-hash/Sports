"""
HybridAnalyzer - ëª¨ë“  ë¶„ì„ ëª¨ë¸ í†µí•©

LLM AI (5ê°œ) + LightGBM + í†µê³„ ì•™ìƒë¸” (4ê°œ)ë¥¼ ê²°í•©í•˜ì—¬
ê°€ì¥ ì •í™•í•œ ì˜ˆì¸¡ì„ ìƒì„±í•©ë‹ˆë‹¤.

ê°€ì¤‘ì¹˜:
- LLM AI ì•™ìƒë¸”: 50%
- LightGBM ML: 25%
- í†µê³„ ì•™ìƒë¸”: 25%
"""

import asyncio
import logging
import os
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict, field
from datetime import datetime

logger = logging.getLogger(__name__)


@dataclass
class ModelPrediction:
    """ê°œë³„ ëª¨ë¸ ì˜ˆì¸¡"""

    model_name: str
    model_type: str  # 'llm', 'ml', 'statistical'
    home_prob: float
    draw_prob: float
    away_prob: float
    confidence: float
    weight: float
    reasoning: str = ""


@dataclass
class HybridResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ê²°ê³¼"""

    # ìµœì¢… í™•ë¥ 
    home_prob: float
    draw_prob: float
    away_prob: float

    # ì˜ˆì¸¡
    predicted_outcome: str  # 'home_win', 'draw', 'away_win'
    predicted_outcome_kr: str  # 'í™ˆìŠ¹', 'ë¬´', 'ì›ì •ìŠ¹'

    # ì‹ ë¢°ë„
    overall_confidence: float
    consensus_score: float  # ëª¨ë¸ ê°„ í•©ì˜ë„

    # ê°œë³„ ëª¨ë¸ ê²°ê³¼
    llm_prediction: Dict
    ml_prediction: Dict
    statistical_prediction: Dict
    all_models: List[ModelPrediction]

    # ë©”íƒ€ë°ì´í„°
    analyzed_at: str
    analysis_time_ms: int

    def to_dict(self) -> Dict:
        result = asdict(self)
        result["all_models"] = [asdict(m) for m in self.all_models]
        return result


class HybridAnalyzer:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ê¸°

    3ê°€ì§€ ë¶„ì„ ê³„ì¸µì„ í†µí•©:
    1. LLM AI ë¶„ì„ (GPT, Claude, Gemini, Kimi, DeepSeek)
    2. LightGBM ML ëª¨ë¸
    3. í†µê³„ ì•™ìƒë¸” (Poisson, ELO, Form, H2H)
    """

    # ê°€ì¤‘ì¹˜ ì„¤ì •
    WEIGHT_LLM = 0.50  # LLM AI 50%
    WEIGHT_ML = 0.25  # LightGBM 25%
    WEIGHT_STAT = 0.25  # í†µê³„ ì•™ìƒë¸” 25%

    def __init__(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        self.llm_available = False
        self.ml_available = False
        self.stat_available = False

        # LLM AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
        try:
            from src.services.ai_orchestrator import AIOrchestrator

            self.ai_orchestrator = AIOrchestrator()
            self.llm_available = len(self.ai_orchestrator.get_active_analyzers()) > 0
            logger.info(
                f"âœ… LLM AI ë¡œë“œ: {len(self.ai_orchestrator.get_active_analyzers())}ê°œ í™œì„±"
            )
        except Exception as e:
            logger.warning(f"âŒ LLM AI ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.ai_orchestrator = None

        # LightGBM ML ëª¨ë¸
        try:
            from src.ml.model import MatchPredictorML

            self.ml_model = MatchPredictorML()
            self.ml_model.load_model()
            if self.ml_model.model:
                self.ml_available = True
                logger.info("âœ… LightGBM ML ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                logger.warning("âš ï¸ LightGBM ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
        except Exception as e:
            logger.warning(f"âŒ LightGBM ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.ml_model = None

        # í†µê³„ ì•™ìƒë¸”
        try:
            from src.services.ensemble_model import EnsembleModel

            self.ensemble_model = EnsembleModel()
            self.stat_available = True
            logger.info("âœ… í†µê³„ ì•™ìƒë¸” ë¡œë“œ (Poisson, ELO, Form, H2H)")
        except Exception as e:
            logger.warning(f"âŒ í†µê³„ ì•™ìƒë¸” ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.ensemble_model = None

        # ê³ ê¸‰ í†µê³„ ì˜ˆì¸¡ê¸°
        try:
            from src.services.predictor import AdvancedStatisticalPredictor

            self.stat_predictor = AdvancedStatisticalPredictor()
        except Exception as e:
            logger.warning(f"âš ï¸ ê³ ê¸‰ í†µê³„ ì˜ˆì¸¡ê¸° ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.stat_predictor = None

        # ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •
        self._adjust_weights()

        logger.info(f"ğŸ“Š HybridAnalyzer ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(
            f"   ê°€ì¤‘ì¹˜: LLM={self.WEIGHT_LLM:.0%}, ML={self.WEIGHT_ML:.0%}, STAT={self.WEIGHT_STAT:.0%}"
        )

    def _adjust_weights(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ë™ì  ì¡°ì •"""
        total_available = 0

        if self.llm_available:
            total_available += self.WEIGHT_LLM
        if self.ml_available:
            total_available += self.WEIGHT_ML
        if self.stat_available:
            total_available += self.WEIGHT_STAT

        if total_available == 0:
            logger.error("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            return

        # ì¬ì •ê·œí™”
        if not self.llm_available:
            self.WEIGHT_LLM = 0
        if not self.ml_available:
            self.WEIGHT_ML = 0
        if not self.stat_available:
            self.WEIGHT_STAT = 0

        # í•©ì´ 1ì´ ë˜ë„ë¡ ì¡°ì •
        total = self.WEIGHT_LLM + self.WEIGHT_ML + self.WEIGHT_STAT
        if total > 0:
            self.WEIGHT_LLM /= total
            self.WEIGHT_ML /= total
            self.WEIGHT_STAT /= total

    async def analyze(
        self,
        match_context: Dict,
        team_stats: Optional[Dict] = None,
        h2h_data: Optional[Dict] = None,
    ) -> HybridResult:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ë¶„ì„ ì‹¤í–‰

        Args:
            match_context: ê²½ê¸° ì •ë³´
                - match_id: str
                - home_team: str
                - away_team: str
                - league: str
                - match_time: str (optional)

            team_stats: íŒ€ í†µê³„ (optional)
                - home: {avg_gf, avg_ga, avg_sf, avg_sa, form, ...}
                - away: {avg_gf, avg_ga, avg_sf, avg_sa, form, ...}

            h2h_data: ìƒëŒ€ ì „ì  (optional)
                - home_wins: int
                - away_wins: int
                - draws: int

        Returns:
            HybridResult
        """
        start_time = datetime.now()
        all_predictions: List[ModelPrediction] = []

        # ê¸°ë³¸ í†µê³„ ì¤€ë¹„
        home_stats = team_stats.get("home", {}) if team_stats else {}
        away_stats = team_stats.get("away", {}) if team_stats else {}

        # 1. LLM AI ë¶„ì„ (ë¹„ë™ê¸°)
        llm_result = await self._analyze_with_llm(match_context, home_stats, away_stats)
        if llm_result:
            all_predictions.append(llm_result)

        # 2. LightGBM ML ë¶„ì„
        ml_result = self._analyze_with_ml(match_context, home_stats, away_stats)
        if ml_result:
            all_predictions.append(ml_result)

        # 3. í†µê³„ ì•™ìƒë¸” ë¶„ì„
        stat_results = self._analyze_with_statistical(home_stats, away_stats, h2h_data)
        all_predictions.extend(stat_results)

        # 4. ê²°ê³¼ í•©ì„±
        final_result = self._synthesize_predictions(all_predictions)

        # 5. ë©”íƒ€ë°ì´í„° ì¶”ê°€
        end_time = datetime.now()
        analysis_time_ms = int((end_time - start_time).total_seconds() * 1000)

        # ê°œë³„ ëª¨ë¸ ê²°ê³¼ ì •ë¦¬
        llm_pred = {
            "available": llm_result is not None,
            "prediction": asdict(llm_result) if llm_result else None,
        }

        ml_pred = {
            "available": ml_result is not None,
            "prediction": asdict(ml_result) if ml_result else None,
        }

        stat_pred = {
            "available": len(stat_results) > 0,
            "predictions": [asdict(r) for r in stat_results],
        }

        return HybridResult(
            home_prob=final_result["home"],
            draw_prob=final_result["draw"],
            away_prob=final_result["away"],
            predicted_outcome=final_result["outcome"],
            predicted_outcome_kr=final_result["outcome_kr"],
            overall_confidence=final_result["confidence"],
            consensus_score=final_result["consensus"],
            llm_prediction=llm_pred,
            ml_prediction=ml_pred,
            statistical_prediction=stat_pred,
            all_models=all_predictions,
            analyzed_at=datetime.now().isoformat(),
            analysis_time_ms=analysis_time_ms,
        )

    async def _analyze_with_llm(
        self, match_context: Dict, home_stats: Dict, away_stats: Dict
    ) -> Optional[ModelPrediction]:
        """LLM AI ë¶„ì„"""

        if not self.llm_available or not self.ai_orchestrator:
            return None

        try:
            # MatchContext ìƒì„±
            from src.services.ai.models import (
                MatchContext,
                SportType as AIModelSportType,
            )

            # sport_type ë³€í™˜
            sport_str = match_context.get("sport_type", "soccer")
            if sport_str in ["basketball", "ë†êµ¬"]:
                sport_enum = AIModelSportType.BASKETBALL
            else:
                sport_enum = AIModelSportType.SOCCER

            context = MatchContext(
                match_id=match_context.get("match_id", 0)
                if isinstance(match_context.get("match_id"), int)
                else 0,
                home_team=match_context.get("home_team", ""),
                away_team=match_context.get("away_team", ""),
                league=match_context.get("league", ""),
                start_time=match_context.get(
                    "match_time", match_context.get("start_time", "")
                ),
                sport_type=sport_enum,
                home_stats=home_stats or None,
                away_stats=away_stats or None,
            )

            # AI ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í˜¸ì¶œ
            result = await self.ai_orchestrator.analyze_match(context)

            if result and result.consensus:
                probs = result.consensus.probabilities or {}
                return ModelPrediction(
                    model_name="LLM AI Ensemble",
                    model_type="llm",
                    home_prob=probs.get("home", 0.33),
                    draw_prob=probs.get("draw", 0.33),
                    away_prob=probs.get("away", 0.33),
                    confidence=result.consensus.confidence / 100
                    if result.consensus.confidence > 1
                    else result.consensus.confidence,
                    weight=self.WEIGHT_LLM,
                    reasoning=result.consensus.recommendation or "",
                )

        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ì˜¤ë¥˜: {e}")

        return None

    def _analyze_with_ml(
        self, match_context: Dict, home_stats: Dict, away_stats: Dict
    ) -> Optional[ModelPrediction]:
        """LightGBM ML ë¶„ì„"""

        if not self.ml_available or not self.ml_model:
            return None

        try:
            probs = self.ml_model.predict_proba(
                home_stats={
                    "avg_gf": home_stats.get(
                        "avg_gf", home_stats.get("goals_scored_avg", 1.5)
                    ),
                    "avg_ga": home_stats.get(
                        "avg_ga", home_stats.get("goals_conceded_avg", 1.0)
                    ),
                    "avg_sf": home_stats.get("avg_sf", 12),
                    "avg_sa": home_stats.get("avg_sa", 10),
                },
                away_stats={
                    "avg_gf": away_stats.get(
                        "avg_gf", away_stats.get("goals_scored_avg", 1.2)
                    ),
                    "avg_ga": away_stats.get(
                        "avg_ga", away_stats.get("goals_conceded_avg", 1.5)
                    ),
                    "avg_sf": away_stats.get("avg_sf", 10),
                    "avg_sa": away_stats.get("avg_sa", 12),
                },
                league_name=match_context.get("league", "Unknown"),
            )

            if probs:
                return ModelPrediction(
                    model_name="LightGBM",
                    model_type="ml",
                    home_prob=probs.get("home", 0.33),
                    draw_prob=probs.get("draw", 0.33),
                    away_prob=probs.get("away", 0.33),
                    confidence=0.75,  # ML ëª¨ë¸ ê¸°ë³¸ ì‹ ë¢°ë„
                    weight=self.WEIGHT_ML,
                    reasoning="LightGBM ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì¸¡",
                )

        except Exception as e:
            logger.error(f"ML ë¶„ì„ ì˜¤ë¥˜: {e}")

        return None

    def _analyze_with_statistical(
        self, home_stats: Dict, away_stats: Dict, h2h_data: Optional[Dict]
    ) -> List[ModelPrediction]:
        """í†µê³„ ì•™ìƒë¸” ë¶„ì„"""

        results = []

        if not self.stat_available or not self.ensemble_model:
            return results

        try:
            # ì•™ìƒë¸” ëª¨ë¸ í˜¸ì¶œ
            ensemble_pred = self.ensemble_model.predict(
                home_avg_goals=home_stats.get(
                    "avg_gf", home_stats.get("goals_scored_avg", 1.5)
                ),
                away_avg_goals=away_stats.get(
                    "avg_gf", away_stats.get("goals_scored_avg", 1.2)
                ),
                home_avg_conceded=home_stats.get(
                    "avg_ga", home_stats.get("goals_conceded_avg", 1.0)
                ),
                away_avg_conceded=away_stats.get(
                    "avg_ga", away_stats.get("goals_conceded_avg", 1.5)
                ),
                home_elo=home_stats.get("elo", 1500),
                away_elo=away_stats.get("elo", 1500),
                home_form=home_stats.get("form", ""),
                away_form=away_stats.get("form", ""),
                h2h_home_wins=h2h_data.get("home_wins", 0) if h2h_data else 0,
                h2h_away_wins=h2h_data.get("away_wins", 0) if h2h_data else 0,
                h2h_draws=h2h_data.get("draws", 0) if h2h_data else 0,
                h2h_home_goals=h2h_data.get("home_goals", 0) if h2h_data else 0,
                h2h_away_goals=h2h_data.get("away_goals", 0) if h2h_data else 0,
            )

            if ensemble_pred:
                # ì „ì²´ ì•™ìƒë¸” ê²°ê³¼
                results.append(
                    ModelPrediction(
                        model_name="Statistical Ensemble",
                        model_type="statistical",
                        home_prob=ensemble_pred.home_prob,
                        draw_prob=ensemble_pred.draw_prob,
                        away_prob=ensemble_pred.away_prob,
                        confidence=ensemble_pred.confidence,
                        weight=self.WEIGHT_STAT,
                        reasoning=f"í•©ì˜ë„: {ensemble_pred.agreement_score:.0%}",
                    )
                )

                # ê°œë³„ ëª¨ë¸ ê²°ê³¼ë„ ì¶”ê°€ (ê°€ì¤‘ì¹˜ 0 - ì°¸ì¡°ìš©)
                for mp in ensemble_pred.model_predictions:
                    results.append(
                        ModelPrediction(
                            model_name=mp.model_name,
                            model_type="statistical_sub",
                            home_prob=mp.home_prob,
                            draw_prob=mp.draw_prob,
                            away_prob=mp.away_prob,
                            confidence=mp.confidence,
                            weight=0,  # ì°¸ì¡°ìš©ì´ë¯€ë¡œ ê°€ì¤‘ì¹˜ 0
                            reasoning="",
                        )
                    )

        except Exception as e:
            logger.error(f"í†µê³„ ë¶„ì„ ì˜¤ë¥˜: {e}")

        # ê³ ê¸‰ í†µê³„ ì˜ˆì¸¡ê¸° ì‚¬ìš©
        if self.stat_predictor and home_stats and away_stats:
            try:
                stat_result = self.stat_predictor.predict_score_probabilities(
                    home_stats={
                        "goals_scored_avg": home_stats.get("avg_gf", 1.5),
                        "goals_conceded_avg": home_stats.get("avg_ga", 1.0),
                        "momentum": home_stats.get("momentum", 1.0),
                    },
                    away_stats={
                        "goals_scored_avg": away_stats.get("avg_gf", 1.2),
                        "goals_conceded_avg": away_stats.get("avg_ga", 1.5),
                        "momentum": away_stats.get("momentum", 1.0),
                    },
                )

                if stat_result:
                    probs = stat_result.get("probabilities", {})
                    results.append(
                        ModelPrediction(
                            model_name="Poisson Distribution",
                            model_type="statistical_sub",
                            home_prob=probs.get("home", 0.33),
                            draw_prob=probs.get("draw", 0.33),
                            away_prob=probs.get("away", 0.33),
                            confidence=0.70,
                            weight=0,  # ì´ë¯¸ ì•™ìƒë¸”ì— í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ
                            reasoning=f"ì˜ˆìƒ ìŠ¤ì½”ì–´: {stat_result.get('expected_score', {})}",
                        )
                    )
            except Exception as e:
                logger.warning(f"ê³ ê¸‰ í†µê³„ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")

        return results

    def _synthesize_predictions(self, predictions: List[ModelPrediction]) -> Dict:
        """ëª¨ë“  ì˜ˆì¸¡ í•©ì„±"""

        if not predictions:
            return {
                "home": 0.33,
                "draw": 0.34,
                "away": 0.33,
                "outcome": "draw",
                "outcome_kr": "ë¬´",
                "confidence": 0.5,
                "consensus": 0.5,
            }

        # ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ì˜ˆì¸¡ë§Œ í•„í„°
        weighted_predictions = [p for p in predictions if p.weight > 0]

        if not weighted_predictions:
            weighted_predictions = predictions[:3]  # ìƒìœ„ 3ê°œ ì‚¬ìš©
            for p in weighted_predictions:
                p.weight = 1.0 / len(weighted_predictions)

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_weight = sum(p.weight for p in weighted_predictions)

        home_prob = (
            sum(p.home_prob * p.weight for p in weighted_predictions) / total_weight
        )
        draw_prob = (
            sum(p.draw_prob * p.weight for p in weighted_predictions) / total_weight
        )
        away_prob = (
            sum(p.away_prob * p.weight for p in weighted_predictions) / total_weight
        )

        # ì •ê·œí™”
        total = home_prob + draw_prob + away_prob
        if total > 0:
            home_prob /= total
            draw_prob /= total
            away_prob /= total

        # ì˜ˆì¸¡ ê²°ê³¼
        probs = {"home": home_prob, "draw": draw_prob, "away": away_prob}
        outcome = max(probs, key=probs.get)
        outcome_map = {
            "home": ("home_win", "í™ˆìŠ¹"),
            "draw": ("draw", "ë¬´"),
            "away": ("away_win", "ì›ì •ìŠ¹"),
        }

        # ì‹ ë¢°ë„: ê°€ì¤‘ í‰ê·  (0~1 ë²”ìœ„ë¡œ ì œí•œ)
        confidence = (
            sum(p.confidence * p.weight for p in weighted_predictions) / total_weight
        )
        # ì‹ ë¢°ë„ê°€ 1ë³´ë‹¤ í° ê²½ìš° (ì˜ˆ: í¼ì„¼íŠ¸ë¡œ ì£¼ì–´ì§„ ê²½ìš°) ì •ê·œí™”
        if confidence > 1:
            confidence = confidence / 100
        confidence = max(0, min(1, confidence))  # 0~1 ë²”ìœ„ë¡œ í´ë¨í•‘

        # í•©ì˜ë„: í‘œì¤€í¸ì°¨ ê¸°ë°˜
        import statistics

        if len(weighted_predictions) >= 2:
            home_probs = [p.home_prob for p in weighted_predictions]
            std = statistics.stdev(home_probs)
            consensus = max(0, 1 - std * 3)
        else:
            consensus = 0.7

        return {
            "home": round(home_prob, 4),
            "draw": round(draw_prob, 4),
            "away": round(away_prob, 4),
            "outcome": outcome_map[outcome][0],
            "outcome_kr": outcome_map[outcome][1],
            "confidence": round(confidence, 4),
            "consensus": round(consensus, 4),
        }

    async def analyze_batch(
        self,
        matches: List[Dict],
        team_stats_map: Optional[Dict[str, Dict]] = None,
        h2h_map: Optional[Dict[str, Dict]] = None,
        max_concurrent: int = 5,
    ) -> List[HybridResult]:
        """ì—¬ëŸ¬ ê²½ê¸° ì¼ê´„ ë¶„ì„"""

        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_with_limit(match: Dict) -> HybridResult:
            async with semaphore:
                match_id = match.get("match_id", "")
                team_stats = (
                    team_stats_map.get(match_id, {}) if team_stats_map else None
                )
                h2h = h2h_map.get(match_id, {}) if h2h_map else None
                return await self.analyze(match, team_stats, h2h)

        results = await asyncio.gather(
            *[analyze_with_limit(m) for m in matches], return_exceptions=True
        )

        # ì˜ˆì™¸ ì²˜ë¦¬
        valid_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"ê²½ê¸° {i + 1} ë¶„ì„ ì˜¤ë¥˜: {result}")
                continue
            valid_results.append(result)

        return valid_results

    def get_model_status(self) -> Dict:
        """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
        return {
            "llm_ai": {
                "available": self.llm_available,
                "weight": self.WEIGHT_LLM,
                "models": self.ai_orchestrator.get_active_analyzers()
                if self.ai_orchestrator
                else [],
            },
            "ml": {
                "available": self.ml_available,
                "weight": self.WEIGHT_ML,
                "model": "LightGBM" if self.ml_available else None,
            },
            "statistical": {
                "available": self.stat_available,
                "weight": self.WEIGHT_STAT,
                "models": ["Poisson", "ELO", "Form", "H2H"]
                if self.stat_available
                else [],
            },
        }

    def get_status(self) -> Dict:
        """ëª¨ë¸ ìƒíƒœ ê°„ë‹¨ ìš”ì•½ (í˜¸í™˜ì„±ìš©)"""
        return {
            "ai_orchestrator": self.llm_available,
            "ml_predictor": self.ml_available,
            "ensemble_model": self.stat_available,
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_hybrid_analyzer: Optional[HybridAnalyzer] = None


def get_hybrid_analyzer() -> HybridAnalyzer:
    """ì‹±ê¸€í†¤ HybridAnalyzer ë°˜í™˜"""
    global _hybrid_analyzer
    if _hybrid_analyzer is None:
        _hybrid_analyzer = HybridAnalyzer()
    return _hybrid_analyzer


# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import asyncio

    logging.basicConfig(level=logging.INFO)

    async def test():
        analyzer = HybridAnalyzer()

        # ëª¨ë¸ ìƒíƒœ í™•ì¸
        status = analyzer.get_model_status()
        print("\nğŸ“Š ëª¨ë¸ ìƒíƒœ:")
        print(
            f"  LLM AI: {status['llm_ai']['available']} ({status['llm_ai']['weight']:.0%})"
        )
        print(f"  ML: {status['ml']['available']} ({status['ml']['weight']:.0%})")
        print(
            f"  í†µê³„: {status['statistical']['available']} ({status['statistical']['weight']:.0%})"
        )

        # í…ŒìŠ¤íŠ¸ ë¶„ì„
        result = await analyzer.analyze(
            match_context={
                "match_id": "test_001",
                "home_team": "Manchester United",
                "away_team": "Liverpool",
                "league": "Premier League",
            },
            team_stats={
                "home": {"avg_gf": 1.8, "avg_ga": 1.0, "form": "WWDLW"},
                "away": {"avg_gf": 2.2, "avg_ga": 0.8, "form": "WWWWW"},
            },
            h2h_data={"home_wins": 3, "away_wins": 5, "draws": 2},
        )

        print(f"\nğŸ¯ ë¶„ì„ ê²°ê³¼:")
        print(f"  í™ˆìŠ¹: {result.home_prob:.1%}")
        print(f"  ë¬´: {result.draw_prob:.1%}")
        print(f"  ì›ì •ìŠ¹: {result.away_prob:.1%}")
        print(f"  ì˜ˆì¸¡: {result.predicted_outcome_kr}")
        print(f"  ì‹ ë¢°ë„: {result.overall_confidence:.1%}")
        print(f"  í•©ì˜ë„: {result.consensus_score:.1%}")
        print(f"  ë¶„ì„ ì‹œê°„: {result.analysis_time_ms}ms")

    asyncio.run(test())
