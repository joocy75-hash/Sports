"""
AI Analysis Orchestrator

ë‹¤ì¤‘ AI ì„œë¹„ìŠ¤ë¥¼ ê´€ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ì§‘ê³„í•˜ëŠ” ì¤‘ì•™ ì„œë¹„ìŠ¤
- ë³‘ë ¬ AI í˜¸ì¶œ (asyncio.gather)
- ê²°ê³¼ ê°€ì¤‘ í‰ê·  ê³„ì‚°
- ì‹ ë¢°ë„ ë° ì»¨ì„¼ì„œìŠ¤ ì‚°ì •
- ìºì‹± ë° ì—ëŸ¬ í•¸ë“¤ë§
"""

import asyncio
import logging
import time
import hashlib
from typing import Dict, List, Optional, Tuple
from datetime import datetime

from .ai.base_analyzer import BaseAIAnalyzer
from .ai.gpt_analyzer import GPTAnalyzer
from .ai.kimi_analyzer import KimiAnalyzer
from .ai.claude_analyzer import ClaudeAnalyzer
from .ai.gemini_analyzer import GeminiAnalyzer
from .ai.deepseek_analyzer import DeepSeekAnalyzer
from .ai.models import (
    AIOpinion,
    AIAnalysisResult,
    ConsensusResult,
    MatchContext,
    WinnerType,
    ConfidenceLevel,
)

logger = logging.getLogger(__name__)


# AI ê°€ì¤‘ì¹˜ ì„¤ì • (5ê°œ AI ëª¨ë¸)
AI_WEIGHTS = {
    "gpt": 0.25,      # GPT-4o: OpenAI ì£¼ë ¥ ëª¨ë¸
    "claude": 0.25,   # Claude: Anthropic ë…¼ë¦¬ì  ì¶”ë¡ 
    "gemini": 0.20,   # Gemini: Google ë¹ ë¥¸ ë¶„ì„
    "deepseek": 0.15, # DeepSeek: ì‹¬ì¸µ ë¶„ì„
    "kimi": 0.15,     # Kimi: ë³´ì¡° ë¶„ì„
}


class AIOrchestrator:
    """
    ë‹¤ì¤‘ AI ë¶„ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

    ì—¬ëŸ¬ AI ì„œë¹„ìŠ¤ë¥¼ ë³‘ë ¬ë¡œ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬
    ì‹ ë¢°ë„ ë†’ì€ ì»¨ì„¼ì„œìŠ¤ ê²°ê³¼ë¥¼ ìƒì„±
    """

    def __init__(self):
        self.analyzers: Dict[str, BaseAIAnalyzer] = {}
        self.cache: Dict[str, AIAnalysisResult] = {}  # ë©”ëª¨ë¦¬ ìºì‹œ (ì‹¤ì œë¡œëŠ” Redis ê¶Œì¥)
        self.cache_ttl = 3600  # 1ì‹œê°„ ìºì‹œ

        self._init_analyzers()

    def _init_analyzers(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™” (5ê°œ AI ëª¨ë¸)"""
        # ëª¨ë“  AI ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        analyzers_config = [
            ("gpt", GPTAnalyzer(), "GPT-4o"),
            ("claude", ClaudeAnalyzer(), "Claude Sonnet"),
            ("gemini", GeminiAnalyzer(), "Gemini 1.5 Flash"),
            ("deepseek", DeepSeekAnalyzer(), "DeepSeek V3"),
            ("kimi", KimiAnalyzer(), "Kimi K2"),
        ]

        for name, analyzer, display_name in analyzers_config:
            if analyzer.is_available():
                self.analyzers[name] = analyzer
                logger.info(f"[Orchestrator] {display_name} Analyzer í™œì„±í™”")
            else:
                logger.warning(f"[Orchestrator] {display_name} Analyzer ë¹„í™œì„± - API í‚¤ ì—†ìŒ")

        active_count = len(self.analyzers)
        if active_count == 0:
            logger.error("[Orchestrator] í™œì„±í™”ëœ AI ë¶„ì„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤!")
        else:
            logger.info(f"[Orchestrator] ì´ {active_count}ê°œ AI ë¶„ì„ê¸° í™œì„±í™”")

    def get_active_analyzers(self) -> List[str]:
        """í™œì„±í™”ëœ ë¶„ì„ê¸° ëª©ë¡"""
        return list(self.analyzers.keys())

    async def analyze_match(
        self,
        context: MatchContext,
        use_cache: bool = True,
    ) -> AIAnalysisResult:
        """
        ë‹¨ì¼ ê²½ê¸° ë¶„ì„

        Args:
            context: ê²½ê¸° ì»¨í…ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            AIAnalysisResult: ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        start_time = time.time()
        cache_key = self._generate_cache_key(context)

        # ìºì‹œ í™•ì¸
        if use_cache and cache_key in self.cache:
            cached = self.cache[cache_key]
            cached.cached = True
            logger.info(f"[Orchestrator] ìºì‹œ íˆíŠ¸: match_id={context.match_id}")
            return cached

        # ë³‘ë ¬ AI í˜¸ì¶œ
        opinions = await self._call_all_analyzers(context)

        if not opinions:
            logger.error(f"[Orchestrator] ë¶„ì„ ì‹¤íŒ¨: match_id={context.match_id}")
            return self._create_empty_result(context)

        # ì»¨ì„¼ì„œìŠ¤ ê³„ì‚°
        consensus = self._calculate_consensus(opinions)

        # ê²°ê³¼ ìƒì„±
        total_latency = int((time.time() - start_time) * 1000)
        result = AIAnalysisResult(
            match_id=context.match_id,
            consensus=consensus,
            ai_opinions=opinions,
            cached=False,
            cache_key=cache_key,
            total_latency_ms=total_latency,
        )

        # ìºì‹œ ì €ì¥
        if use_cache:
            self.cache[cache_key] = result

        logger.info(
            f"[Orchestrator] ë¶„ì„ ì™„ë£Œ: match_id={context.match_id}, "
            f"consensus={consensus.winner.value}, "
            f"confidence={consensus.confidence}%, "
            f"latency={total_latency}ms"
        )

        return result

    async def analyze_batch(
        self,
        contexts: List[MatchContext],
        use_cache: bool = True,
        max_concurrent: int = 5,
    ) -> List[AIAnalysisResult]:
        """
        ì—¬ëŸ¬ ê²½ê¸° ì¼ê´„ ë¶„ì„

        Args:
            contexts: ê²½ê¸° ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            max_concurrent: ë™ì‹œ ì²˜ë¦¬ ìˆ˜

        Returns:
            List[AIAnalysisResult]: ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def analyze_with_semaphore(ctx: MatchContext) -> AIAnalysisResult:
            async with semaphore:
                return await self.analyze_match(ctx, use_cache)

        tasks = [analyze_with_semaphore(ctx) for ctx in contexts]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ì˜ˆì™¸ ì²˜ë¦¬
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"[Orchestrator] ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜: {result}")
                final_results.append(self._create_empty_result(contexts[i]))
            else:
                final_results.append(result)

        return final_results

    async def _call_all_analyzers(self, context: MatchContext) -> List[AIOpinion]:
        """ëª¨ë“  ë¶„ì„ê¸° ë³‘ë ¬ í˜¸ì¶œ"""
        if not self.analyzers:
            return []

        tasks = {
            name: analyzer.analyze_match(context)
            for name, analyzer in self.analyzers.items()
        }

        results = await asyncio.gather(*tasks.values(), return_exceptions=True)

        opinions = []
        for name, result in zip(tasks.keys(), results):
            if isinstance(result, Exception):
                logger.error(f"[{name}] ë¶„ì„ ì‹¤íŒ¨: {result}")
            elif result.confidence > 0:  # ìœ íš¨í•œ ê²°ê³¼ë§Œ í¬í•¨
                opinions.append(result)
            else:
                logger.warning(f"[{name}] ë¶„ì„ ê²°ê³¼ ë¬´íš¨ (confidence=0)")

        return opinions

    def _calculate_consensus(self, opinions: List[AIOpinion]) -> ConsensusResult:
        """
        ë‹¤ì¤‘ AI ì˜ê²¬ ì¢…í•©

        ê°€ì¤‘ í‰ê·  ë°©ì‹ìœ¼ë¡œ í™•ë¥  ê³„ì‚°
        ì¼ì¹˜ë„ì— ë”°ë¼ ì‹ ë¢°ë„ ì¡°ì •
        """
        if not opinions:
            return self._create_default_consensus()

        # ê°€ì¤‘ í™•ë¥  ê³„ì‚°
        weighted_probs = {"home": 0.0, "draw": 0.0, "away": 0.0}
        total_weight = 0.0

        for opinion in opinions:
            weight = AI_WEIGHTS.get(opinion.provider, 0.5)
            confidence_factor = opinion.confidence / 100  # ì‹ ë¢°ë„ ë°˜ì˜

            if opinion.probabilities:
                for key in weighted_probs:
                    weighted_probs[key] += (
                        opinion.probabilities.get(key, 0.33) * weight * confidence_factor
                    )
                total_weight += weight * confidence_factor

        # ì •ê·œí™”
        if total_weight > 0:
            for key in weighted_probs:
                weighted_probs[key] /= total_weight

        # ìŠ¹ì ê²°ì •
        winner_key = max(weighted_probs, key=weighted_probs.get)
        winner_map = {"home": WinnerType.HOME, "draw": WinnerType.DRAW, "away": WinnerType.AWAY}
        winner = winner_map[winner_key]

        # ì¼ì¹˜ë„ ê³„ì‚°
        agreement_count = sum(1 for op in opinions if op.winner == winner)
        agreement_rate = agreement_count / len(opinions) if opinions else 0

        # ê°€ì¤‘ ì‹ ë¢°ë„ ê³„ì‚°
        weighted_confidence = 0.0
        weight_sum = 0.0
        for opinion in opinions:
            weight = AI_WEIGHTS.get(opinion.provider, 0.5)
            weighted_confidence += opinion.confidence * weight
            weight_sum += weight

        avg_confidence = int(weighted_confidence / weight_sum) if weight_sum > 0 else 50

        # ì¼ì¹˜ë„ì— ë”°ë¥¸ ì‹ ë¢°ë„ ì¡°ì •
        if agreement_rate == 1.0:  # ë§Œì¥ì¼ì¹˜
            adjusted_confidence = min(avg_confidence + 10, 100)
        elif agreement_rate >= 0.5:  # ê³¼ë°˜ìˆ˜ ì¼ì¹˜
            adjusted_confidence = avg_confidence
        else:  # ë¶„ì‚°
            adjusted_confidence = max(avg_confidence - 10, 30)

        # ì‹ ë¢°ë„ ìˆ˜ì¤€ ê²°ì •
        if adjusted_confidence >= 80:
            confidence_level = ConfidenceLevel.HIGH
        elif adjusted_confidence >= 60:
            confidence_level = ConfidenceLevel.MEDIUM
        elif adjusted_confidence >= 40:
            confidence_level = ConfidenceLevel.LOW
        else:
            confidence_level = ConfidenceLevel.UNCERTAIN

        # ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±
        recommendation = self._generate_recommendation(winner, adjusted_confidence, agreement_rate)

        return ConsensusResult(
            winner=winner,
            confidence=adjusted_confidence,
            confidence_level=confidence_level,
            probabilities=weighted_probs,
            agreement_rate=agreement_rate,
            recommendation=recommendation,
        )

    def _generate_recommendation(
        self, winner: WinnerType, confidence: int, agreement_rate: float
    ) -> str:
        """ì¶”ì²œ ë©”ì‹œì§€ ìƒì„±"""
        winner_korean = {"Home": "í™ˆ ìŠ¹ë¦¬", "Draw": "ë¬´ìŠ¹ë¶€", "Away": "ì›ì • ìŠ¹ë¦¬"}
        winner_text = winner_korean.get(winner.value, "ë¬´ìŠ¹ë¶€")

        if agreement_rate == 1.0 and confidence >= 70:
            return f"ğŸ”¥ ê°•ë ¥ ì¶”ì²œ: {winner_text} (AI ë§Œì¥ì¼ì¹˜, ì‹ ë¢°ë„ {confidence}%)"
        elif agreement_rate >= 0.5 and confidence >= 60:
            return f"âœ… ì¶”ì²œ: {winner_text} (ì‹ ë¢°ë„ {confidence}%)"
        elif confidence >= 50:
            return f"âš ï¸ ì°¸ê³ : {winner_text} ì˜ˆìƒ (ì‹ ë¢°ë„ {confidence}%)"
        else:
            return f"â“ ë¶ˆí™•ì‹¤: {winner_text} ì•½ì„¸ ì˜ˆìƒ (ì‹ ë¢°ë„ ë‚®ìŒ)"

    def _generate_cache_key(self, context: MatchContext) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{context.match_id}_{context.home_team}_{context.away_team}_{datetime.now().strftime('%Y%m%d')}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _create_default_consensus(self) -> ConsensusResult:
        """ê¸°ë³¸ ì»¨ì„¼ì„œìŠ¤ ìƒì„±"""
        return ConsensusResult(
            winner=WinnerType.DRAW,
            confidence=0,
            confidence_level=ConfidenceLevel.UNCERTAIN,
            probabilities={"home": 0.33, "draw": 0.34, "away": 0.33},
            agreement_rate=0,
            recommendation="ë¶„ì„ ë¶ˆê°€",
        )

    def _create_empty_result(self, context: MatchContext) -> AIAnalysisResult:
        """ë¹ˆ ê²°ê³¼ ìƒì„±"""
        return AIAnalysisResult(
            match_id=context.match_id,
            consensus=self._create_default_consensus(),
            ai_opinions=[],
            cached=False,
        )

    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache.clear()
        logger.info("[Orchestrator] ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")

    def get_cache_stats(self) -> Dict:
        """ìºì‹œ í†µê³„"""
        return {
            "size": len(self.cache),
            "keys": list(self.cache.keys())[:10],  # ìµœê·¼ 10ê°œë§Œ
        }
